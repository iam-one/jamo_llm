torchrun train.py --batch_size=64 --with_lr_scheduler --wandb